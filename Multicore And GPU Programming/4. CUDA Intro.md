CUDA는 Compute Unified Device Architecture의 약자로 GPU 프로그래밍 모델이다. 

#### GPU Architecture
##### Streaming Multiprocessor
보통 SM이라고 부르는 Streaming Multiprocessor는 GPU에 여러 개 존재한다. 이는 다수의 CUDA 코어를 가진 연산 장치로 NVIDIA GTX 1080 아키텍처의 경우 SM에 32개의 CUDA 코어를 가지고 있다.
CUDA 코어는 GPU의 가장 기본적인 연산을 수행하는 기본 단위이다. CUDA 코어 하나가 하나의 스레드를 처리하게 된다.
##### CUDA thread 계층
우리가 GPU 커널을 `kernel_func<<<dimGrid, dimBlock>>>()`의 형태로 실행하게 된다. 따라서 커널을 실행하면 이렇게 전달된 레이아웃에 따라 Grid가 생성된다. 
그리드에 포함된 **thread block은 그리드가 배정된 GPU 속 SM에 의해 처리된다**. 즉, thread block을 처리하는 단위는 그리드라고 할 수 있다. 만약 CUDA 프로그램이 8개의 thread block을 가진다고 가정하자. 이때,  GPU에 SM이 2개가 있다면 SM마다 4개의 thread block이 할당되고, GPU에 SM이 4개 있다면 SM마다 2개의 block이 할당된다. 여기서 알 수 있듯이, SM마다 여러 개의 thread block을 가질 수 있으며, SM이 사용할 수 있는 자원의 한계와 한 thread block을 처리하기 위해 필요한 자원의 양에 따라 한 SM이 동시에 처리할 수 있는 thread block의 수가 결정된다. 
Thread block 안의 thread들은 SM안에서 동시에 실행되는데 이는 실행되는 block 간의 순서가 보장되지 않는다는 얘기다. 또, SM당 사용할 수 있는 thread의 개수가 정해져있다. 따라서, thread block이 많으면 동시에 모든 thread block을 실행하지 못할 경우가 생긴다. 결국 SM이 많을 수록 각 SM당 할당되는 thread block의 개수가 적어지고, 각 thread block이 동시에 SM 안에서 실행될 가능성이 더 높아지고 이는 더 나은 성능을 제공한다.
또, thread block 안의 thread들이 동시에 실행되고 각 thread들은 서로 자원을 공유한다. 이는 shared memory와 `_syncthread` 를 통해 가능하고 thread block 간에는 서로 소통할 수 없다. 즉, 각 thread block은 보장된 순서 없이 독립적으로 실행된다.
이제 warp라는 개념이 나오는데 이는 hardware dependent하다. NVIDIA가 어떻게 정해주냐에 따라 달라지고 현재는 warp라는 단위는 32개의 CUDA core를 포함한다. Thread block에서도 32개의 thread를 가진 warp로 분할된다. 따라서 대체로 SM내의 CUDA core 수가 32의 배수로 설정되는데 이는 warp size를 고려한 아키텍처이다. 그렇다면 만약 thread block의 thread 수를 32의 배수가 아닌 수로 설정하게 되면 warp 단위로 thread가 실행되기 때문에 idle 상태인 thread가 발생하게 되고 이는 SM내의 thread를 fully utilization하지 못하기 때문에 성능 저해가 일어날 수 밖에 없다.
warp는 warp scheduler에 의해 움직이고 warp 내 thread가 모두 ready 상태일 경우에 실행된다. warp 내 thread들은 독립적으로 실행 가능하며 각 thread는 자신만의 context를 갖는다. 이는 GPU 아키텍처 상 CPU보다 많고 큰 register를 갖고 있기 때문이고  각 register가 warp 내 thread에 배정된다. 따라서 context switching overhead가 발생하지 않는다. 따라서 하나의 명령어에 대해 warp내 모든 thread가 여러 개의 데이터를 처리하므로 이를 SIMD, SIMT(Single Instruction Multiple Threading)이라 부른다.

**Registers**: many... why? 
Size of shared memroy > L1 cahce -> why?

How GPU change context every cycle
: we don't have to back up register -> fast context switching(no overhead)


SM(Streaming Multiprocessor)
- Each thread blocks are allocated to SM
- Utilize all of the cores :128 -> every thread to small cores

**32 units (cuda cores) Executes one warp at a time**
warp : 32 thread (unit of execution (together) in GPU) -> execution on lock step
- SIMD

**Thread Scheduling**
- warp size <- NVIDIA design에 의해 변경가능
![[Pasted image 20250417130917.png|400]]
- 4 warps can be executed in parallel.
- *Ready*
- thread stall -> GPU change thread to one of *Ready* thread inside sm
![[Pasted image 20250417131437.png | 400]]
- 7 cannot be execute(e.g. add), after instruction 6(load)
- 여기서 TB1의 W1, W2, W3의 progress는 다 다름.
- Running warp cannot execute next instruction -> change TB1 to TB2
- Instruction 3 (TB2) cannot be execute
- Change running thread to any Ready WARP
-> no wasting time executing SM
- Assumption : we need to have bunch of warps to each SM. $\rightarrow$ stall 될 때마다 바꿀 Ready 상태인 warp 있어야 한다.
- 한번에 하나의 warp만 실행가능
- The SM keeps all the information needed for the warps within the SM -> Register file이 한다.
	$\rightarrow$ No context switching
- Warp의 모든 thread는 같은 instruction 실행 $\leftarrow$ SIMD

threads -> warp -> threadblock -> Grid

barrier() == `_syncthread` : match the progress of function with each threads
- Every threads in a warp are executing same instruction
- btw different warps 동기화하기 위해
- 각 warp는 다른 prgoress가지고 있음

In-reality : actual physical GPU
IF-ELSE : thread마다 다를 수 있음 -> barnch divergence to handle branch inside warp
CPU creates mask(predication)
- collect threads with same condition -> low utilization(performance degradation)

### Avoiding branch divergence
- No branch divergence
- Execution time of each time look similar
	- warp 1 stalled after 7 cycle
	- warp 2 stalle dafter 7 cycle
	- ...
- Because, basically the same program
- 