행렬 곱셈을 수식으로 나타내면 다음과 같다.

$$
C_{i,j} =\sum_k A_{ik}\cdot B_{kj}
$$
이 때 Matrix $A, B, C$는 모두 $N\times N$ Matrix라고 가정한다. 위 식은 $C$ Matrix의 한 element를 $A$의 row elements와 $B$의 모든 column elements를 곱하여 더한 값으로 나타낼 수 있다는 것을 표현하고 있다. 이를 다음과 같은 식으로 나타낼 수 있다.
$$
c_{i,j} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{iN}b_{Nj}
$$
이제부터 이 행렬 $C$를 System의 하드웨어 특징과 아키텍처 및 멀티스레딩을 통해 속도를 개선하는 과정을 거칠 것이다.
그렇다면 우선 Naive한 버전의 Matrix Multiplication의 pseudo-code를 살펴보면 다음과 같다. 
```
for i = 1 to n
	for j = 1 to n
		for k = 1 to n
			C(i, j) = C(i, j) + A(i, k) * B(k, j)
```
이는 그냥 Naive하게 행렬 곱셈을 구현한 것이기 때문에 당연히 좋은 성능을 내지 못한다. 그럼 일단 Parallel programming을 다루는 수업이기 때문에 병렬화 시켜서 계산해 보는 방법을 생각해볼 수 있다.
#### Try 1. Columnwise block stripping
이는 행렬 $C$의 column을 나눠서 thread에 할당하는 방식이다. pseudo code는 다음과 같다.
```
for i = 1 to n
	for j = 1 to n step P
		for k = 1 to n
			C(i,j) = C(i, j) + A(i, k) * B(k, j)
```
P는 thread의 개수를 뜻한다.  편의상 P개의 thread의 tid를 각각 1~P까지 부여해보자. 이때 `tid=1`인 thread는 $C$ Matrix의 1번째, $P+1$번째, $2P+1$번째 $\dots$ $n-P-1$ 번째 column을 계산하게 된다. 이렇게 각각의 thread가 맡는 $C$의 column이 있으므로 P개씩 넘어가면서 계산하게 된다. 이때의 그리고 각 thread 간의 데이터 의존성이 없으므로 race condition이 발생하지 않아 lock이 필요없다. 이 행렬 계산의 시간복잡도는 $N^3 / P$ 가 되는데 그 이유는 뭘까. 기존 Naive한 행렬 곱셈에서는 $C$ element하나를 계산하는데에 $A$의 $N$개의 element와 $B$의 $N$개의 element를 각각 곱하고 더하게 되므로 $N$ 의 시간복잡도가 발생한다. 이 과정이 $N$의 길이를 가지는 $C$의 row를 계산하는 데에 각각 진행하게 되므로 $N^2$이 된다. $C$의 column의 길이 또한 $N$이므로 $N^3$의 시간 복잡도를 가지게 된다. 하지만 이 방법에서는 N개의 column을 P개의 thread가 맡게 되므로 $N^2 \times N/P = N^3/P$ 의 시간 복잡도까지 감소하게 된다. 
하지만 이 방법 또한 보완할 점 그리고 단점이 존재한다.
1. False sharing이 발생할 수 있다. 
	 - 보통 CPU는 접근한 데이터를 cache line(보통 64KB) 단위로 cache에 데이터를 올린다. 이는 row-wise로 저장되기 때문에 column-wise로 각 thread들에게 책임을 분산하게 되면 (cache line이 N개의 row만큼의 크기를 가진다고 했을 때) 1번 thread가 column 1에 대한 계산을 한다고 해도 cache line은 row-wise로 저장되기 때문에 1번 thread가 데이터를 수정했으면 2번 thread가 계산할 차례가 됐을 때는 같은 cache line에 있는 데이터를 1번 thread가 수정했기 때문에 다시 메모리에서 데이터를 불러와야 한다. 이 과정은 같은 cache line을 공유하는 다른 thread들 차례가 왔을 때 똑같이 일어나며 memory read를 계속 하게 되므로 시간 지연이 발생하게 된다. 그 이유는 CPU는 cache value consistency를 위해 coherence protocol을 사용하게 되며 cache line을 수정할 때마다 다른 코어에 invalidation 신호를 보내게 된다. 이 때문에 실제로는 thread간에 명시적인 communication 과정은 없지만 cache coherence traffic이 계속해서 발생하게 된다. 이는 곧 **Significant Perofrmance bug** 결과를 낳는다.
> [!Note] False Sharing
> Thread들이 서로 다른 변수를 사용하지만, 해당 변수들이 같은 cache line에 있어 불필요한 cache 동기화가 일어나는 현상을 말한다.

위의 방식처럼 column wise로 성능을 개선하려는 경우에는 false sharing으로 인해 성능 저하가 발생한다. 이는 cache line을 row-wise로 저장하기 때문이었고 자연스럽게 row-wise로 $C$를 구분해서 thread에게 맡기면 될 것 같다는 생각으로 이어질 수 있다.
#### Try 2. Rowwise block stripping
이는 행렬 $C$를 $P$개의 row로 나눠서 thread에 할당하는 방식이다. pseudo code는 다음과 같다.
```
for i = 1 to n step P
	for j = 1 to n
		for k = 1 to n
			C(i,j) = C(i, j) + A(i, k) * B(k, j)
```
이 방식 또한 thread 간 데이터 의존성이 없기 때문에 race condition이 발생하지 않아 lock이 필요 없으며 한 thread가 담당하고 있는 elements들이 같은 cache line에 놓여있기 때문에 false sharing 또한 발생하지 않는다. 이 방법 또한 $N^3/P$의 시간복잡도를 가지게 된다. 이제 다시 이 방법이 완벽하다고 할 수 있을까? 라는 질문을 던질 수 있다.

---
## Consider Caches
이전에 데이터가 처음 메모리에서 로드될 때 근처 데이터와 함께 cache line단위로 불러 와진다는 것을 언급한 적이 있다. Memory에 접근해 데이터를 로드하는 것보다는 cache에 있는 데이터를 불러오는 것이 빠르다는 것을 알기 때문에 우리는 이제 cache 크기를 고려하여 데이터 로드 속도를 빠르게 하여 행렬 곱셈 계산을 더 빨리 할 수 있다는 것을 알 수 있다. 이를 알아보자.
다음 구조를 통해 데이터 로드할 때 어떤 하드웨어를 먼저 접근하는지를 알 수 있다.

![[Pasted image 20250403222057.png]]

L1 cache가 가장 빠르고, 메모리가 가장 느리다는 것을 알 수 있다. 코어 하나당 L1, L2 cache가 있고 멀티 코어 아키텍처에서는 각 코어가 L3 캐시를 공유하는 구조를 띄고 있다. 당연히 L1 cache에서 데이터를 접근하는 것이 가장 빠르기 때문에 우리는 L1 cache를 살펴볼 필요가 있다. 
L1 cache는 32KB~64KB 크기를 가지며 float 자료형의 데이터 크기는 4B이기 때문에 L1 cache에 모든 데이터를 담는다고 가정했을 때 8K~16KB까지의 float 데이터를 저장할 수 있다. 따라서 Matrix 크기에 이 구조를 대입해 생각해보면 $N\times N$ matrix에서 각 element가 float 자료형이라고 했을 때 한번에 $N=128$ 의 행렬을 L1 cache에 집어넣을 수 있다. ($2^7 \times 2^7 \times 2^2 = 2^{16} = 64KB$). 하지만 이는 이상적으로 생각했을 때의 결과이다. 현실적으로는, $A,B,C$ 행렬, 실행 결과, 코드 실행 스택 등도 L1 cache에 올라가기 때문에 64KB 정도가 최대로 들어갈 수 있는 정도다.
Cache에 들어있는 데이터를 접근했을 때를 cache hit라고 한다. 그리고 데이터가 없어서 메모리에서 다시 로드해야 되는 상황을 cache miss라고 한다. 이때 cache hit의 비율을 높여야 당연히 실행 속도를 높일 수 있을 것이다.

> [! Note] Cache Miss 종류
> 1. Cold Miss: 처음 접근하는 데이터에 대해서 발생하는 miss.
> 2. Capacity Miss: 캐시 용량 부족으로 인해 오래된 데이터가 밀려나고 다시 접근하 때 발생하는 cache miss
> 3. Conflict Miss: 서로 다른 주소가 같은 캐시 슬롯에 매핑되어 충돌 발생 (Direct-mapped 캐시에서 주로 발생)

따라서 우리는 cache miss가 언제 발생하는 지를 먼저 파악해볼 필요가 있다. Single thread 버전에서 Matrix multiplication을 진행할 때 cache miss가 언제 발생하는 지 먼저 관찰해보자.
![[Pasted image 20250403225655.png | 200]]
이 상황에서는 $A, B$ 모두 데이터에 접근하기 때문에 모두 cold miss가 발생한다. 그 다음
![[Pasted image 20250403225800.png | 200]]
이 상황이 오면 $A$에서 다음 데이터에 접근할 때는 모두 cache hit이 발생하게 되고 $B$는 여전히 cold miss가 발생한다.
이렇게 되면 $C$의 row하나를 모두 완성하게 되기까지 $A$는 cold miss 한 번, $B$에서는 $N$번의 cold miss가 발생하게 된다.
$C$의 다음 row를 계산할 때는 $B$의 열 전체가 L1 cache에 fit하지 않기 때문에 계속 miss가 발생하게 되고 이는 capacity miss이다. 이미 cache가 다 차있는 상태일 것이다. 이전에 접근한 $B$의 row는 다른 데이터에 의해 대체 돼있을 것이다(`evict`). 따라서 다시 이전 데이터에 접근할 때는 메모리에서 다시 데이터를 로드해와야 할 것이고 cache는 이미 꽉 차있기 때문에 capacity loss가 발생하는 것이다.
결과적으로 메모리 접근 횟수는 다음과 같다.
- $A : N^2 / c$ reads
- $B: N^3 / c$ reads
- C : $N^2 /c$ reads + $N^2 / c$ writes 
