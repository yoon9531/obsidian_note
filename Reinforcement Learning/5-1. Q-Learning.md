기존 Policy gradient는 보상을 통해 확률적으로 정책을 업데이트하기 때문에 gradient의 분산이 매우 커서 학습이 불안정한 경우가 많다. 반면, Q-Learning은 value-base 방법이기 때문에 high variance 문제로부터 자유로우며 이로 인해 안정적인 학습이 가능하다.
