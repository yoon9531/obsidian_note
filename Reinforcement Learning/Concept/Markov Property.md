어떤 상태 $s_t$ 가 Markov property를 만족하기 위해서는 다음 조건을 만족해야 한다.
$$
P(s_{t+1} \mid s_t) = P(s_{t+1} \mid s_1, \dots, s_t)
$$
식을 의미를 해석해보면 미래 상태 $s_{t+1}$은 현재 상태 $s_t$ 에만 의존하고 이전 상태는 미래 상태에 전혀 영향을 주지 못한다는 뜻이다. 이는 현재 상태가 과거 상태의 정보를 모두 담고 있다는 의미를 내포하고 있다. 체스를 예시로 들면 현재 체스판에 있는 말의 위치(상태)를 보고 다음 수(미래 상태)를 예측할 수 있다는 의미이다.

강화학습에서는 환경을 [[Markov Decision Process]]로 모델링한다. 이때의 전제 조건은 모든 상태가 Markov Property를 만족하고 있다는 것이다. 이를 통해 MDP에서 정책을 $\pi(a_t \mid s_t)$ 형태로 정의할 수 있다.