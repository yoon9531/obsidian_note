## ğŸ“– Definition

Markov Decision ProcessëŠ” ë‹¤ìŒ tupleë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
$$
\left<S,  A,p(s_0), p(s_{t+1} \mid s_t, a_t), r(s_t, a_t), \gamma\right>
$$
MRPì™€ì˜ ì°¨ì´ë¼ë©´, ì—ì´ì „íŠ¸ì˜ ì¡´ì¬ ìœ ë¬´ì´ë‹¤. ë³´ìƒì˜ ëˆ„ì í•©ì„ ìµœëŒ€ë¡œ í•œë‹¤ëŠ” ëª©ì ì€ ê°™ì§€ë§Œ Agentì˜ ì˜ì‚¬ê²°ì •ì„ í†µí•œ í–‰ë™ì´ ì¶”ê°€ëœë‹¤. ì´ëŠ” ìœ„ íŠœí”Œì—ì„œ $A$ì™€ $a_t$ê°€ ì¶”ê°€ ëœ ê²ƒìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- $S$ : set of states
- $A$ : set of actions
- $p(s_0)$ : Initial state distribution
- $p(s_{t+1} \mid s_t, a_t)$ : state transition í™•ë¥ 
- $r(s_t, a_t)$ : reward function, í–‰ë™ $a_t$ ë¥¼ ì·¨í•œ í›„ ìƒíƒœ $s_t$ ì—ì„œ ì–»ëŠ” ë³´ìƒ
- $\gamma$ : discount factor

## ğŸ“ Key Concepts

- Markov Propertyì „
	- Future stateëŠ” í˜„ì¬ stateì— ì˜í•´ì„œë§Œ ê²°ì •ëœë‹¤. 
	- = Future stateëŠ” ê³¼ê±° stateì— ë…ë¦½ì ì´ë‹¤.
	- = stateëŠ” ê¸°ë¡ì— ë‚¨ê²¨ì ¸ ìˆëŠ” ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ì €ì¥í•œë‹¤.
	- If the state is **Markov** if and only if
$$
p(s_{t+1} \mid s_t) = p(s_{t+1} \mid s_1, \dots , s_t)
$$
- ì „ì œ ì¡°ê±´ì€ ëª¨ë“  stateê°€ Markov propertyë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì´ë‹¤
### í•œ ì‚¬ì´í´ì˜ íë¦„ 
- ì—ì´ì „íŠ¸ëŠ” ìƒíƒœ $s_t$â€‹ë¥¼ ê´€ì°°
    
- ì •ì±… $\pi(a_t | s_t)$ì— ë”°ë¼ í–‰ë™ $a_t$â€‹ ì„ íƒ
    
- í™˜ê²½ì€ ë‹¤ìŒ ìƒíƒœ $s_{t+1}$ê³¼ ë³´ìƒ $r_t$ë¥¼ ë°˜í™˜
    
- ì—ì´ì „íŠ¸ëŠ” ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ
    
- ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ëˆ„ì  ë³´ìƒ í•©ì´ ìµœëŒ€ê°€ ë˜ë„ë¡í•œë‹¤.
## ğŸ—ƒï¸ Related Topics

- [[Markov Reward Process]]
    
- [[Markov Property]]    

## â“ Common Questions

## ğŸ§  Summary in One Sentence

## ğŸ“… Dates

- First studied: 2025-04-01
    
- Last reviewed:
    

## ğŸ·ï¸ Tags

#Senior #Markov #RL 